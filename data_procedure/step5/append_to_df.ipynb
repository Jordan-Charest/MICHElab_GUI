{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf63690",
   "metadata": {},
   "source": [
    "Set parameters and select segmenting signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3e3917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox_jocha.hdf5 import get_data_from_dataset\n",
    "from toolbox_jocha.ets import split_into_bins\n",
    "\n",
    "# \"308-8\", \"308-10\", \"308-12\", \"308-14\", \"316-8\", \"316-10\"\n",
    "# \"316-12\", \"322-6\", \"322-8\", \"322-10\", \"322-12\", \"353-6\"    \n",
    "# \"353-8\", \"353-10\", \"361-6\", \"365-6\", \"367-6\", \"374-6\"\n",
    "# \"374-8\", \"374-10\", \"387-6\", \"387-10\", \"396-6\", \"397-6\"\n",
    "# \"410-6\", \"410-8\", \"410-10\", \"412-8\", \"412-10\", \"415-6\", \"415-8\"\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "mice_num = [\"39-12\",\"42-12\",\"44-12\",\"45-12\",\"46-12\",\"251-6\",\"254-6\"]\n",
    "output_file_id = \"noise_v1\"\n",
    "GCaMP_str = \"GCaMP_shuffled\"\n",
    "\n",
    "indexes = [f\"M{mouse_num}_{output_file_id}\" for mouse_num in mice_num]\n",
    "\n",
    "base_dataframe_filename = \"WT_shuffled_noise_df.csv\"\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "# mice_num = [\"396-1_3fps_GCaMP_highpass_HbT_infraslow\"]\n",
    "# indexes = [f\"M{mouse_num}\" for mouse_num in mice_num]\n",
    "\n",
    "#############################\n",
    "\n",
    "n_mice = len(mice_num)\n",
    "n_segments = 2\n",
    "\n",
    "segmenting_str = GCaMP_str\n",
    "\n",
    "def signals_filename(mouse_num, filename_str):\n",
    "    return f\"D:/mouse_data/new_data/M{mouse_num}/formatted/M{mouse_num}_{filename_str}.h5\"\n",
    "\n",
    "def dfc_filename(mouse_num, filename_str, signal_str):\n",
    "    return f\"D:/mouse_data/new_data/M{mouse_num}/formatted/M{mouse_num}_{filename_str}_{signal_str}_dfc.h5\"\n",
    "\n",
    "##################################\n",
    "\n",
    "# def signals_filename(mouse_num, filename_str):\n",
    "#     return f\"D:/mouse_data/new_data/M396-1/formatted/M{mouse_num}.h5\"\n",
    "\n",
    "# def dfc_filename(mouse_num, filename_str, signal_str):\n",
    "#     return f\"D:/mouse_data/new_data/M396-1/formatted/M{mouse_num}_{signal_str}_dfc.h5\"\n",
    "\n",
    "###################################\n",
    "\n",
    "\n",
    "segment_mats = []\n",
    "segment_indices = []\n",
    "\n",
    "for mouse_num in mice_num:\n",
    "    \n",
    "    cts, _ = get_data_from_dataset(dfc_filename(mouse_num, output_file_id, segmenting_str), \"cts\")\n",
    "\n",
    "    mats, indices = split_into_bins(cts, n_segments)\n",
    "\n",
    "    segment_mats.append(mats)\n",
    "    segment_indices.append(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab26baf",
   "metadata": {},
   "source": [
    "Read existing dataframe to add data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9db435c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 number\n",
      "id                     \n",
      "M39-12_noise_v1      39\n",
      "M42-12_noise_v1      42\n",
      "M44-12_noise_v1      44\n",
      "M45-12_noise_v1      45\n",
      "M46-12_noise_v1      46\n",
      "M251-6_noise_v1     251\n",
      "M254-6_noise_v1     254\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def return_dataframe(filename):\n",
    "    return pd.read_csv(filename).set_index(\"id\")\n",
    "\n",
    "def copy_dataframe(input_file, output_file):\n",
    "    if not os.path.exists(output_file):\n",
    "        shutil.copyfile(input_file, output_file)\n",
    "\n",
    "def update_dataframe(df, data, columns, indexes):\n",
    "    if data.ndim == 1:\n",
    "        data = np.array([data]).T  # Ensure 2D\n",
    "\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    for i, idx in enumerate(indexes):\n",
    "        if idx in df.index:\n",
    "            # Update existing row using Series and df.update\n",
    "            for col_idx, col in enumerate(columns):\n",
    "                df.at[idx, col] = data[i, col_idx]\n",
    "        else:\n",
    "            # Create a new row dictionary\n",
    "            new_row = {col: data[i, col_idx] for col_idx, col in enumerate(columns)}\n",
    "            df.loc[idx] = new_row  # This adds a new row\n",
    "\n",
    "    return df\n",
    "\n",
    "df = return_dataframe(base_dataframe_filename)\n",
    "print(df)\n",
    "\n",
    "# data = np.array([1, 2])\n",
    "# cols = [\"Col R\"]\n",
    "# indexes = [\"M308-8_v1\", \"M316-8_v1\"]\n",
    "\n",
    "# df = update_dataframe(df, data, cols, indexes)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599ca56",
   "metadata": {},
   "source": [
    "1. Neurovascular coupling (correlation between GCaMP activity and dHbT activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "604145cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mouse M39-12.\n",
      "Processing mouse M42-12.\n",
      "Processing mouse M44-12.\n",
      "Processing mouse M45-12.\n",
      "Processing mouse M46-12.\n",
      "Processing mouse M251-6.\n",
      "Processing mouse M254-6.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from toolbox_jocha.correlation import r_coeff_2mats\n",
    "from toolbox_jocha.hdf5 import get_data_from_dataset\n",
    "\n",
    "filename = \"nvc_\"+base_dataframe_filename\n",
    "\n",
    "copy_dataframe(base_dataframe_filename, filename)\n",
    "df = return_dataframe(filename)\n",
    "\n",
    "\n",
    "squared_r = False\n",
    "compute_lag = True\n",
    "max_shift_seconds = 5\n",
    "fps = 3\n",
    "convert_to_s = True\n",
    "lag_sign = None\n",
    "\n",
    "max_shift = max_shift_seconds * fps\n",
    "neurovascular_coupling = np.zeros((n_mice, n_segments))\n",
    "neurcoup_whole = np.zeros(n_mice)\n",
    "lag = np.zeros((n_mice, n_segments))\n",
    "lag_whole = np.zeros(n_mice)\n",
    "\n",
    "for i, mouse_num in enumerate(mice_num):\n",
    "\n",
    "    print(f\"Processing mouse M{mouse_num}.\")\n",
    "\n",
    "    GCaMP_signal, _ = get_data_from_dataset(signals_filename(mouse_num, output_file_id), f\"data/3d/{GCaMP_str}\")\n",
    "    HbT_signal, _ = get_data_from_dataset(signals_filename(mouse_num, output_file_id), \"data/3d/dHbT\")\n",
    "\n",
    "    lag_mat, correlation_mat = r_coeff_2mats(GCaMP_signal, HbT_signal, max_shift=max_shift, lag=compute_lag, convert_to_s=convert_to_s, fps=fps, squared=squared_r, lag_sign=lag_sign)\n",
    "    neurcoup_whole[i] = np.nanmean(correlation_mat)\n",
    "    lag_whole[i] = np.nanmean(lag_mat)\n",
    "\n",
    "    for j, indices in enumerate(segment_indices[i]): # The j-th segment of mouse i\n",
    "\n",
    "        sliced_GCaMP_signal = GCaMP_signal[indices,:,:]\n",
    "        sliced_HbT_signal = HbT_signal[indices,:,:]\n",
    "\n",
    "        lag_mat, correlation_mat = r_coeff_2mats(sliced_GCaMP_signal, sliced_HbT_signal, max_shift=max_shift, lag=compute_lag, convert_to_s=convert_to_s, fps=fps, squared=squared_r, lag_sign=lag_sign)\n",
    "\n",
    "        neurovascular_coupling[i,j] = np.nanmean(correlation_mat)\n",
    "        lag[i,j] = np.nanmean(lag_mat)\n",
    "\n",
    "\n",
    "# mouse i's neurovascular coupling (as a list) is given by\n",
    "# neurovasc_i = list(neurovascular_coupling[i,:])\n",
    "\n",
    "# We want to append it to the file as nvc_{segmenting_str}segmented_{n_segments}segments\n",
    "\n",
    "# print(neurovascular_coupling)\n",
    "# print(lag)\n",
    "\n",
    "# Seems to be about 8 seconds per segment with 5 segments\n",
    "\n",
    "nvc_columns = [\"nvc_whole\"] + [f\"nvc_segment_{i}/{n_segments}\" for i in range(n_segments)]\n",
    "df = update_dataframe(df, np.column_stack((neurcoup_whole, neurovascular_coupling)), nvc_columns, indexes)\n",
    "\n",
    "lag_columns = [\"lag_whole\"] + [f\"lag_segment_{i}/{n_segments}\" for i in range(n_segments)]\n",
    "df = update_dataframe(df, np.column_stack((lag_whole, lag)), lag_columns, indexes)\n",
    "\n",
    "# df[f\"nvc_{n_segments}_{segmenting_str}_segments\"].update(pd.Series(neurovascular_coupling.tolist(), index=indexes))\n",
    "# df[f\"nvc_whole\"].update(pd.Series(neurcoup_whole.tolist(), index=indexes))\n",
    "\n",
    "# df[f\"lag_{n_segments}_{segmenting_str}_segments\"].update(pd.Series(lag.tolist(), index=indexes))\n",
    "# df[f\"lag_whole\"].update(pd.Series(lag_whole.tolist(), index=indexes))\n",
    "\n",
    "df.to_csv(filename)\n",
    "\n",
    "del neurovascular_coupling, neurcoup_whole, lag, lag_whole, GCaMP_signal, HbT_signal, lag_mat, correlation_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6865936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional representativity\n",
    "funcrep_filename = \"funcrep_\"+base_dataframe_filename\n",
    "copy_dataframe(base_dataframe_filename, funcrep_filename)\n",
    "funcrep_df = return_dataframe(funcrep_filename)\n",
    "\n",
    "# Modularity\n",
    "modularity_filename = \"modularity_\"+base_dataframe_filename\n",
    "copy_dataframe(base_dataframe_filename, modularity_filename)\n",
    "modularity_df = return_dataframe(modularity_filename)\n",
    "\n",
    "# Functional similarity\n",
    "funcsim_filename = \"funcsim_\"+base_dataframe_filename\n",
    "copy_dataframe(base_dataframe_filename, funcsim_filename)\n",
    "funcsim_df = return_dataframe(funcsim_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffddbe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mouse M39-12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\torte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3202: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mouse M42-12.\n",
      "Processing mouse M44-12.\n",
      "Processing mouse M45-12.\n",
      "Processing mouse M46-12.\n",
      "Processing mouse M251-6.\n",
      "Processing mouse M254-6.\n"
     ]
    }
   ],
   "source": [
    "import bct\n",
    "import numpy as np\n",
    "\n",
    "GCaMP_FC_whole = [np.nan for i in mice_num] # A\n",
    "dHbT_FC_whole = [np.nan for i in mice_num] # B\n",
    "\n",
    "funcsim_whole = np.zeros(n_mice) # C\n",
    "functional_similarity = np.zeros((n_mice, n_segments)) # D\n",
    "\n",
    "GCaMP_functional_representativity = np.zeros((n_mice, n_segments)) # E\n",
    "dHbT_functional_representativity = np.zeros((n_mice, n_segments)) # F\n",
    "\n",
    "GCaMP_mod_whole = np.zeros(n_mice) # G\n",
    "dHbT_mod_whole = np.zeros(n_mice) # H\n",
    "GCaMP_modularity = np.zeros((n_mice, n_segments)) # I\n",
    "dHbT_modularity = np.zeros((n_mice, n_segments)) # J\n",
    "\n",
    "def flat_to_symmetric(flat, N):\n",
    "    \"\"\"Convert a flattened upper triangle vector to a full symmetric matrix.\"\"\"\n",
    "    mat = np.zeros((N, N))\n",
    "    inds = np.triu_indices(N)\n",
    "    mat[inds] = flat\n",
    "    mat[(inds[1], inds[0])] = flat  # Reflect upper triangle to lower\n",
    "    return mat\n",
    "\n",
    "def compute_modularity(fc, n_elems):\n",
    "    N = int((np.sqrt(8*n_elems+1)-1)/2)\n",
    "    sym_FC = flat_to_symmetric(fc, N)\n",
    "    Ci, Q = bct.modularity_und(sym_FC)\n",
    "    return Q\n",
    "\n",
    "\n",
    "\n",
    "for i, mouse_num in enumerate(mice_num):\n",
    "    \n",
    "    print(f\"Processing mouse M{mouse_num}.\")\n",
    "\n",
    "    GCaMP_dfc, _ = get_data_from_dataset(dfc_filename(mouse_num, output_file_id, GCaMP_str), \"dfc\")\n",
    "    dHbT_dfc, _ = get_data_from_dataset(dfc_filename(mouse_num, output_file_id, \"dHbT\"), \"dfc\")\n",
    "\n",
    "    GCaMP_FC_whole[i] = np.mean(GCaMP_dfc, axis=0) # A\n",
    "    dHbT_FC_whole[i] = np.mean(dHbT_dfc, axis=0) # B\n",
    "\n",
    "    funcsim_whole[i] = np.corrcoef(GCaMP_FC_whole[i], dHbT_FC_whole[i])[0,1] # C\n",
    "\n",
    "    GCaMP_mod_whole[i] = compute_modularity(GCaMP_FC_whole[i], GCaMP_dfc.shape[1]) # G\n",
    "    dHbT_mod_whole[i] = compute_modularity(dHbT_FC_whole[i], dHbT_dfc.shape[1]) # H\n",
    "\n",
    "    for j, indices in enumerate(segment_indices[i]): # The j-th segment of mouse i\n",
    "\n",
    "        sliced_GCaMP_dfc = GCaMP_dfc[indices,:]\n",
    "        sliced_dHbT_dfc = dHbT_dfc[indices,:]\n",
    "\n",
    "        GCaMP_FC = np.mean(sliced_GCaMP_dfc, axis=0)\n",
    "        dHbT_FC = np.mean(sliced_dHbT_dfc, axis=0)\n",
    "\n",
    "        functional_similarity[i,j] = np.corrcoef(GCaMP_FC, dHbT_FC)[0,1] # D\n",
    "\n",
    "        GCaMP_functional_representativity[i,j] = np.corrcoef(GCaMP_FC, GCaMP_FC_whole[i])[0,1] # E\n",
    "        dHbT_functional_representativity[i,j] = np.corrcoef(dHbT_FC, dHbT_FC_whole[i])[0,1] # F\n",
    "\n",
    "        GCaMP_modularity[i,j] = compute_modularity(GCaMP_FC, GCaMP_dfc.shape[1]) # I\n",
    "        dHbT_modularity[i,j] = compute_modularity(dHbT_FC, dHbT_dfc.shape[1]) # J\n",
    "\n",
    "        del sliced_GCaMP_dfc, sliced_dHbT_dfc\n",
    "\n",
    "    del GCaMP_dfc, dHbT_dfc\n",
    "\n",
    "# We're not saving A and B\n",
    "\n",
    "# df[f\"funcsim_whole\"].update(pd.Series(funcsim_whole.tolist(), index=indexes)) # C\n",
    "# df[f\"funcsim_{n_segments}_{segmenting_str}_segments\"].update(pd.Series(functional_similarity.tolist(), index=indexes)) # D\n",
    "\n",
    "funcsim_columns = [\"funcsim_whole\"] + [f\"funcsim_segment_{i}/{n_segments}\" for i in range(n_segments)]\n",
    "funcsim_df = update_dataframe(funcsim_df, np.column_stack((funcsim_whole, functional_similarity)), funcsim_columns, indexes) # C and D\n",
    "funcsim_df.to_csv(funcsim_filename)\n",
    "\n",
    "# df[f\"GCaMP_funcrep_{n_segments}_{segmenting_str}_segments\"].update(pd.Series(GCaMP_functional_representativity.tolist(), index=indexes)) # E\n",
    "# df[f\"dHbT_funcrep_{n_segments}_{segmenting_str}_segments\"].update(pd.Series(dHbT_functional_representativity.tolist(), index=indexes)) # F\n",
    "\n",
    "funcrep_columns = [f\"GCaMP_funcrep_segment_{i}/{n_segments}\" for i in range(n_segments)] + [f\"dHbT_funcrep_segment_{i}/{n_segments}\" for i in range(n_segments)]\n",
    "funcrep_df = update_dataframe(funcrep_df, np.column_stack((GCaMP_functional_representativity, dHbT_functional_representativity)), funcrep_columns, indexes) # E and F\n",
    "funcrep_df.to_csv(funcrep_filename)\n",
    "\n",
    "# df[f\"GCaMP_modularity_whole\"].update(pd.Series(GCaMP_mod_whole.tolist(), index=indexes)) # G\n",
    "# df[f\"dHbT_modularity_whole\"].update(pd.Series(dHbT_mod_whole.tolist(), index=indexes)) # H\n",
    "\n",
    "# df[f\"GCaMP_modularity_{n_segments}_{segmenting_str}_segments\"].update(pd.Series(GCaMP_modularity.tolist(), index=indexes)) # I\n",
    "# df[f\"dHbT_modularity_{n_segments}_{segmenting_str}_segments\"].update(pd.Series(dHbT_modularity.tolist(), index=indexes)) # J\n",
    "\n",
    "modularity_columns = [\"GCaMP_modularity_whole\"] + [f\"GCaMP_modularity_segment_{i}/{n_segments}\" for i in range(n_segments)] + [\"dHbT_modularity_whole\"] + [f\"dHbT_modularity_segment_{i}/{n_segments}\" for i in range(n_segments)]\n",
    "modularity_df = update_dataframe(modularity_df, np.column_stack((GCaMP_mod_whole, GCaMP_modularity, dHbT_mod_whole, dHbT_modularity)), modularity_columns, indexes) # G, H, I and J\n",
    "modularity_df.to_csv(modularity_filename)\n",
    "\n",
    "# df.to_csv(base_dataframe_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
